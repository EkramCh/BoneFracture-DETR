{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"6Pc4C4-QtQrV","executionInfo":{"status":"error","timestamp":1750874702795,"user_tz":-60,"elapsed":1417,"user":{"displayName":"Ekram Chamseddine","userId":"05806370901877655594"}},"outputId":"8c924387-b9f4-4b1b-df42-e64c78ac41d0"},"id":"6Pc4C4-QtQrV","execution_count":null,"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}]},{"cell_type":"markdown","id":"67024584-d139-4450-b6c6-e35473ebeada","metadata":{"id":"67024584-d139-4450-b6c6-e35473ebeada"},"source":["# DETR - Detection Transformer\n","Reference: https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb\n","\n","\n","Original DETR paper: https://arxiv.org/abs/2005.12872\n","\n","Original DETR repo: https://github.com/facebookresearch/detr\n"]},{"cell_type":"code","execution_count":null,"id":"0facd98c-44d0-41e4-83ea-2b00a63434ce","metadata":{"id":"0facd98c-44d0-41e4-83ea-2b00a63434ce"},"outputs":[],"source":["#!python --version"]},{"cell_type":"code","execution_count":null,"id":"5ac8d340-a033-4eb0-bcea-62da2986ea69","metadata":{"id":"5ac8d340-a033-4eb0-bcea-62da2986ea69"},"outputs":[],"source":["!python -m pip install --upgrade pip\n","\n","!pip install supervision==0.3.0\n","\n","!pip install transformers\n","\n","!pip install pytorch-lightning\n","\n","!pip install timm\n","\n","!pip install cython\n","\n","!pip install pycocotools\n","\n","!pip install scipy\n","\n","!pip install roboflow"]},{"cell_type":"code","execution_count":null,"id":"9972369d-f326-42c8-9acd-2386912af26a","metadata":{"id":"9972369d-f326-42c8-9acd-2386912af26a"},"outputs":[],"source":["!pip --version"]},{"cell_type":"code","execution_count":null,"id":"50de65dc-2100-4abd-b20b-0eaaa1007d19","metadata":{"id":"50de65dc-2100-4abd-b20b-0eaaa1007d19"},"outputs":[],"source":["import torch\n","torch.__version__"]},{"cell_type":"code","execution_count":null,"id":"dd82a917-f1a8-473b-a4b4-f49019191aea","metadata":{"id":"dd82a917-f1a8-473b-a4b4-f49019191aea"},"outputs":[],"source":["import supervision as sv\n","import transformers\n","\n","sv.__version__ , transformers.__version__"]},{"cell_type":"code","execution_count":null,"id":"5d0c1b81-66c5-4076-a664-e154b523e396","metadata":{"id":"5d0c1b81-66c5-4076-a664-e154b523e396"},"outputs":[],"source":["import pytorch_lightning\n","print(pytorch_lightning.__version__)"]},{"cell_type":"markdown","id":"65977a3b-682b-41a5-8fd2-3f203a0f91b2","metadata":{"id":"65977a3b-682b-41a5-8fd2-3f203a0f91b2"},"source":["## Create COCO data loaders\n","\n","### About dataset:\n","\n","#### number of classes - 4  ['angle', 'fracture', 'line', 'messed_up_angle'\n","\n","\n","![image.png](attachment:3c3cf0f7-faa8-477c-b8df-40ff4415b187.png)]"]},{"cell_type":"markdown","id":"f8706435-98d4-4de3-b874-7a61f042767e","metadata":{"id":"f8706435-98d4-4de3-b874-7a61f042767e"},"source":["![image.png](attachment:d4d14b07-49fc-4d16-b0c0-702e6620cd17.png)"]},{"cell_type":"code","execution_count":null,"id":"96bb6f09-6e5b-484d-aa13-d405bdedd212","metadata":{"id":"96bb6f09-6e5b-484d-aa13-d405bdedd212"},"outputs":[],"source":["import os\n","import torchvision\n","from transformers import DetrImageProcessor\n","image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n","\n","dataset = '/content/drive/MyDrive/Vit/Data/BoneFractureYolo8/'\n","\n","ANNOTATION_FILE_NAME = \"annotations.json\"\n","TRAIN_DIRECTORY = os.path.join(dataset, \"train/images\")\n","VAL_DIRECTORY = os.path.join(dataset, \"valid/images\")\n","TEST_DIRECTORY = os.path.join(dataset, \"test/images\")\n","\n","\n","class CocoDetection(torchvision.datasets.CocoDetection):\n","    def __init__(\n","        self,\n","        image_directory_path: str,\n","        image_processor,\n","        train: bool = True\n","    ):\n","        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n","        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n","        self.image_processor = image_processor\n","\n","    def __getitem__(self, idx):\n","        images, annotations = super(CocoDetection, self).__getitem__(idx)\n","        image_id = self.ids[idx]\n","        annotations = {'image_id': image_id, 'annotations': annotations}\n","        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n","        pixel_values = encoding[\"pixel_values\"].squeeze()\n","        target = encoding[\"labels\"][0]\n","\n","        return pixel_values, target\n","\n","\n","TRAIN_DATASET = CocoDetection(image_directory_path=TRAIN_DIRECTORY, image_processor=image_processor, train=True)\n","VAL_DATASET = CocoDetection(image_directory_path=VAL_DIRECTORY, image_processor=image_processor, train=False)\n","TEST_DATASET = CocoDetection(image_directory_path=TEST_DIRECTORY, image_processor=image_processor, train=False)\n","\n","print(\"Number of training examples:\", len(TRAIN_DATASET))\n","print(\"Number of validation examples:\", len(VAL_DATASET))\n","print(\"Number of test examples:\", len(TEST_DATASET))"]},{"cell_type":"code","execution_count":null,"id":"db4132d2-0782-411d-842e-be5c53a04903","metadata":{"id":"db4132d2-0782-411d-842e-be5c53a04903"},"outputs":[],"source":["\n","# Visualize if dataset is loaded properly\n","\n","import random\n","import cv2\n","import numpy as np\n","\n","\n","# select random image\n","image_ids = TRAIN_DATASET.coco.getImgIds()\n","image_id = random.choice(image_ids)\n","print('Image #{}'.format(image_id))\n","\n","# load image and annotatons\n","image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n","annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n","image_path = os.path.join(TRAIN_DATASET.root, image['file_name'])\n","image = cv2.imread(image_path)\n","\n","# annotate\n","detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n","\n","# we will use id2label function for training\n","categories = TRAIN_DATASET.coco.cats\n","id2label = {k: v['name'] for k,v in categories.items()}\n","\n","labels = [\n","    f\"{id2label[class_id]}\"\n","    for _, _, class_id, _\n","    in detections\n","]\n","\n","box_annotator = sv.BoxAnnotator()\n","frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n","\n","%matplotlib inline\n","sv.show_frame_in_notebook(image, (8, 8))"]},{"cell_type":"code","execution_count":null,"id":"5e2a4cb2-1ce1-40ad-8465-65a9e0faf8e3","metadata":{"id":"5e2a4cb2-1ce1-40ad-8465-65a9e0faf8e3"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","def collate_fn(batch):\n","    pixel_values = [item[0] for item in batch]\n","    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n","    labels = [item[1] for item in batch]\n","    return {\n","        'pixel_values': encoding['pixel_values'],\n","        'pixel_mask': encoding['pixel_mask'],\n","        'labels': labels\n","    }\n","\n","TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=4, shuffle=True)\n","VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=4)\n","TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=4)"]},{"cell_type":"markdown","id":"72fcb245-4d1c-42cf-916b-f1541da14b40","metadata":{"id":"72fcb245-4d1c-42cf-916b-f1541da14b40"},"source":["# Train model with PyTorch Lightning\n","\n","The DETR model is loaded using the Hugging Face Transformers library"]},{"cell_type":"code","execution_count":null,"id":"24f05df1-7768-4561-9e2b-803dd58cb209","metadata":{"id":"24f05df1-7768-4561-9e2b-803dd58cb209"},"outputs":[],"source":["import pytorch_lightning as pl\n","from transformers import DetrForObjectDetection\n","import torch\n","\n","\n","class Detr(pl.LightningModule):\n","\n","    def __init__(self, lr, lr_backbone, weight_decay):\n","        super().__init__()\n","        self.model = DetrForObjectDetection.from_pretrained(\n","            pretrained_model_name_or_path=\"facebook/detr-resnet-50\",\n","            num_labels=len(id2label),\n","            ignore_mismatched_sizes=True\n","        )\n","\n","        self.lr = lr\n","        self.lr_backbone = lr_backbone\n","        self.weight_decay = weight_decay\n","\n","    def forward(self, pixel_values, pixel_mask):\n","        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n","\n","    def common_step(self, batch, batch_idx):\n","        pixel_values = batch[\"pixel_values\"]\n","        pixel_mask = batch[\"pixel_mask\"]\n","        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n","\n","        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n","\n","        loss = outputs.loss\n","        loss_dict = outputs.loss_dict\n","\n","        return loss, loss_dict\n","\n","    def training_step(self, batch, batch_idx):\n","        loss, loss_dict = self.common_step(batch, batch_idx)\n","        # logs metrics for each training_step, and the average across the epoch\n","        self.log(\"training_loss\", loss)\n","        for k,v in loss_dict.items():\n","            self.log(\"train_\" + k, v.item())\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss, loss_dict = self.common_step(batch, batch_idx)\n","        self.log(\"validation/loss\", loss)\n","        for k, v in loss_dict.items():\n","            self.log(\"validation_\" + k, v.item())\n","\n","        return loss\n","\n","    def configure_optimizers(self):\n","        # DETR authors decided to use different learning rate for backbone\n","        # you can learn more about it here:\n","        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L22-L23\n","        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L131-L139\n","        param_dicts = [\n","            {\n","                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n","            {\n","                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n","                \"lr\": self.lr_backbone,\n","            },\n","        ]\n","        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n","\n","    def train_dataloader(self):\n","        return TRAIN_DATALOADER\n","\n","    def val_dataloader(self):\n","        return VAL_DATALOADER"]},{"cell_type":"code","execution_count":null,"id":"64cf1368-1f4a-4c08-aefe-3d90d2fda638","metadata":{"id":"64cf1368-1f4a-4c08-aefe-3d90d2fda638"},"outputs":[],"source":["model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n","\n","batch = next(iter(TRAIN_DATALOADER))\n","outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"]},{"cell_type":"code","execution_count":null,"id":"16ec6225-5a31-4cb7-93f6-13584215d75b","metadata":{"id":"16ec6225-5a31-4cb7-93f6-13584215d75b"},"outputs":[],"source":["from pytorch_lightning import Trainer\n","\n","# settings\n","MAX_EPOCHS = 200\n","\n","trainer = Trainer( max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n","\n","trainer.fit(model)"]},{"cell_type":"markdown","id":"0138f3cf-5dd2-4ae6-b200-3db7c4674b21","metadata":{"id":"0138f3cf-5dd2-4ae6-b200-3db7c4674b21"},"source":["# Save and load model"]},{"cell_type":"code","execution_count":null,"id":"609c44c2-7164-435d-a618-6b66a62d5a3e","metadata":{"id":"609c44c2-7164-435d-a618-6b66a62d5a3e"},"outputs":[],"source":["\n","MODEL_PATH = '/content/drive/MyDrive/Vit/Object Detection'\n","model.model.save_pretrained(MODEL_PATH)\n","\n","# loading model\n","model = DetrForObjectDetection.from_pretrained(MODEL_PATH)\n","model.to(DEVICE)"]},{"cell_type":"markdown","id":"d2b200b9-87c7-4993-bddf-18f57969714b","metadata":{"id":"d2b200b9-87c7-4993-bddf-18f57969714b"},"source":["# Inference on test dataset\n","\n","Let's visualize the predictions of DETR on the first image of the validation set."]},{"cell_type":"code","execution_count":null,"id":"af859be2-e1b8-4da4-ad4e-91e54dcc1c8d","metadata":{"id":"af859be2-e1b8-4da4-ad4e-91e54dcc1c8d"},"outputs":[],"source":["import random\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","# utils\n","categories = TEST_DATASET.coco.cats\n","id2label = {k: v['name'] for k,v in categories.items()}\n","box_annotator = sv.BoxAnnotator()\n","\n","# select random image\n","image_ids = TEST_DATASET.coco.getImgIds()\n","image_id = random.choice(image_ids)\n","print('Image #{}'.format(image_id))\n","\n","# load image and annotatons\n","image = TEST_DATASET.coco.loadImgs(image_id)[0]\n","annotations = TEST_DATASET.coco.imgToAnns[image_id]\n","image_path = os.path.join(TEST_DATASET.root, image['file_name'])\n","image = cv2.imread(image_path)\n","\n","# Annotate ground truth\n","detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n","labels = [f\"{id2label[class_id]}\" for _, _, class_id, _ in detections]\n","frame_ground_truth = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n","CONFIDENCE_TRESHOLD = 0.1\n","\n","\n","\n","# Annotate detections\n","with torch.no_grad():\n","\n","    # load image and predict\n","    inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n","    outputs = model(**inputs)\n","\n","    # post-process\n","    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n","    results = image_processor.post_process_object_detection(\n","        outputs=outputs,\n","        threshold=CONFIDENCE_TRESHOLD,\n","        target_sizes=target_sizes\n","    )[0]\n","\n","\n","    detections = sv.Detections.from_transformers(transformers_results=results)\n","    labels = [f\"{id2label[class_id]} {confidence:.2f}\" for _, confidence, class_id, _ in detections]\n","    frame_detections = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n","\n","\n","# %matplotlib inline # Remove this line, we won't use it anymore\n","\n","# Combine both images side by side and display\n","fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n","axs[0].imshow(cv2.cvtColor(frame_ground_truth, cv2.COLOR_BGR2RGB))\n","axs[0].axis('off')\n","axs[0].set_title('Ground Truth')\n","\n","axs[1].imshow(cv2.cvtColor(frame_detections, cv2.COLOR_BGR2RGB))\n","axs[1].axis('off')\n","axs[1].set_title('Detections')\n","\n","plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}